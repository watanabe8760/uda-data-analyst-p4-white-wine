---
title: "Quality and Physicochemical Properties of White Wine"
author: "[Teru Watanabe](https://ph.linkedin.com/in/teruwatanabe)"
date: "Friday, February 12, 2016"
output: html_document
---

```{r global options, include=F}
knitr::opts_chunk$set(fig.width=12, fig.height=8, 
                      echo=T, warning=F, message=F)
```


### Table of Contents
1. Abstract
2. Data
3. Library
4. Exploratory Analysis  
    4.1. Univariate Analysis  
    4.2. Bivariate Analysis  
    4.3. Multivariate Analysis  
5. Modeling  
    5.1. Linear Regression  
    5.2. Support Vector Machine  
    5.3. Random Forest  
6. Final Plots and Summary  
    6.1. Alcohol  
    6.2. Density  
    6.3. Residual Sugar with Density  
7. Reflection
8. Reference


# 1. Abstract
The purpose of this project is to determine what physicochemical properties affect white wine quality through exploratory data analysis. 



# 2. Data
The Wine Quality dataset is downloaded from the following website.

[Website] http://www3.dsi.uminho.pt/pcortez/wine/  
[Data Link] http://www3.dsi.uminho.pt/pcortez/wine/winequality.zip  
[Citation] P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.

The zip file contains two csv files, one for red wines (winequality-red.csv) and one for white wines (winequality-white.csv). In this project only the white wine data is used.

The white wine csv file is loaded into R as follows. 
```{r loading data}
# Load csv file
ww <- read.table("./input/winequality-white.csv", header=T, sep=";")
str(ww)
```
The first eleven variables are physicochemical properties acquired from a computerized system iLab and the last variable `quality` is the score between 0 (very bad) and 10 (excellent) based on sensory data. Each sample was evaluated by a minimum of three sensory assessors (using blind tastes) and the final sensory score was given by the median of these evaluations.


For a sake of convenience of analysis, original variable names and independent variable names (physicochemical properties) are defined.
```{r constants}
# Define original variable names
ORIGINAL <- colnames(ww)
# Define independent variable names
INDEPENDENT <- colnames(ww)[1:11]
```

To enable to look at the quality as a classification, a factor variable which is converted from the numerical one is added.
```{r quality.f}
ww$quality.f <- as.factor(ww$quality)
summary(ww$quality.f)
```



# 3. Library
The following libraries are used in this project.
```{r loading library, echo=T}
# For visualization
library(ggplot2)
library(gridExtra)
library(corrplot)
library(dplyr)
# For modeling
library(psych)
library(memisc)
library(nnet)
library(rpart)
library(e1071)
library(randomForest)
```



# 4. Exploratory Analysis
## 4.1. Univariate Analysis
```{r summary}
summary(ww)
```

All variables except `residual.sugar` have very similar median and mean. This makes us imagine symetrical distributions. Also, by looking at 1st quarter and 3rd quarter, it feels that they are relatively close to the center values. So how do they look like? Blue lines show the mean and red lines how the median of distribution. Also dot-lines are 1st and 3rd quantiles.

```{r densityplot1}
dens <- lapply(ORIGINAL, FUN=function(var) {
  ggplot(ww, aes_string(x=var)) + 
    geom_density(fill='gray') +
    geom_vline(aes(xintercept=mean(ww[, var])), color='blue', size=1) +
    geom_vline(aes(xintercept=median(ww[, var])), color='red', size=1) +
    geom_vline(aes(xintercept=quantile(ww[, var], 0.25)), 
               linetype='dashed', size=0.5) + 
    geom_vline(aes(xintercept=quantile(ww[, var], 0.75)), 
               linetype='dashed', size=0.5)
  })
do.call(grid.arrange, args=c(dens, list(ncol=3)))
```

As expected, most of them, except `residual.sugar` and `alcohol`, have symetrical distribution althoguh some have outliers in positive side. The dependent variiable `quality` is a little bit exceptional since it's a discrete number variable.

To have a better sense about the distributions, let's standardize and see.
```{r scaled}
ww_scaled <- data.frame(scale(ww[, ORIGINAL]))
summary(ww_scaled)
```

Since the 1st and 3rd quarter of standard normal distribution is `r round(qnorm(0.25), 3)` and `r round(qnorm(0.75), 3)` respectively, some of them look really close to it. So let's check the normality by Q-Q plot.

```{r qqplot}
par(mfrow=c(4,3), mar=c(2,2,2,2))
dummy <- lapply(ORIGINAL, FUN=function(var) {
  qqnorm(ww_scaled[, var], main=var)
  qqline(ww_scaled[, var])
})
```

Frankly speaking, all of them follow normal distribution in the duration between -2 and 2 standard deviations. (Again, `quality` is exceptional since it's a discrete number distribution.) But as it gets close to the end of positive, outliers are observed. Only `alcohol` has narrower distribution in the positive side as compared to normal distribution. On the other hand, if you look at negative sides, most of them has narrower distiribution than normal distribution. Those observations imply that distributions of independent variables are close to normal distribution at the center, but outliers in positive sides add a taste of right skewness.


## 4.2. Bivariate Analysis
To take an overview of relationships between two variables, let's start with correlation matrix.
```{r corrplot}
corrplot.mixed(cor(ww[, ORIGINAL]))
```

In terms of relationships between `quality` and independent variables, `alcohol` has the strongest correlation 0.44 with `quality`. The second is `density` -0.31 though, it seems natural since the correlation between `density` and `alcohol` is -0.78. (The `density` does not seem to provide any new information about quality since it's already explained by alcohol.)

In terms of relationships between independent variables, some strong correlations are observed.

* 0.84 `residual.sugar` - `density`
* 0.62 `free.sulfur.dioxide` - `total.sulfur.dioxide`
* 0.53 `total.sulfur.dioxide` - `density`

To see relationships between `quality` and independent variables closely, let's create boxplots which desceribe each independent variable per quality level. Here I remove outliers in the positive side to visualize the mojority clearer.

```{r boxplot1}
showBoxplt <- function(df, y, x='quality.f', lower=0, upper=.99) {
  ggplot(df, aes_string(x=x, y=y, fill=x)) + 
    geom_boxplot() + 
    ylim(quantile(df[, y], prob=lower), 
         quantile(df[, y], prob=upper)) +
    theme(legend.position="none")
}
boxp1 <- lapply(INDEPENDENT, FUN=function(var) showBoxplt(ww, var))
do.call(grid.arrange, args=c(boxp1, list(ncol=3)))
```

By looking at the center lines and position of boxes, we can feel the correlation in `alcohol` and `density` which we found in the correlation matrix. As `quality` goes up, `alcohol` also goes up and `density` goes down. Meanwhile the others look almost flat.

```{r boxplot1-1, fig.height=4, echo=F}
boxp.idx1 <- which(INDEPENDENT == 'alcohol' | INDEPENDENT == 'density')
do.call(grid.arrange, args=c(boxp1[boxp.idx1], list(ncol=2)))
```

We can see a loose trend in `alcohol` and `density`.

```{r boxplot1-2, fig.height=4, echo=F}
boxp.idx2 <- which(INDEPENDENT == 'pH' | INDEPENDENT == 'sulphates')
do.call(grid.arrange, args=c(boxp1[boxp.idx2], list(ncol=2)))
```

But for the others, for example `pH` and `sulphates` shown above look flat and no distiction among quality levels. Remember, quality 9 which looks a little different has just five data points.

Since it's hard to find significant factor at this point, let's create a new variable which has fewer quality levels and see if it can provide new insights.
```{r quality.f2}
# [Quality conversion 1]
# Create a simpler classification of quality.
#   3, 4, 5 -> bad
#      6    -> normal
#   7, 8, 9 -> good
ww$quality.f2 <- ifelse(ww$quality == 3 | 
                          ww$quality == 4 | 
                          ww$quality == 5, 
                        "bad", 
                        ifelse(ww$quality == 6, 
                               "normal",
                               "good"))
ww$quality.f2 <- factor(ww$quality.f2, levels=c("bad", "normal", "good"))
summary(ww[, c('quality.f', 'quality.f2')])
```

So now how do boxplots look like?

```{r boxplot2}
boxp2 <- lapply(INDEPENDENT, 
                FUN=function(var) showBoxplt(ww, var, 'quality.f2'))
do.call(grid.arrange, args=c(boxp2, list(ncol=3)))
```

While significant differences among quality levels in `density` and `alcohol` got clearer, the others got flatter. This feels like that the other physicochemical properties do not really matter for quality.

The same zoom-up goes as follows.

```{r boxplot2-1, fig.height=4, echo=F}
do.call(grid.arrange, args=c(boxp2[boxp.idx1], list(ncol=2)))
```

They show a nice hierarchical distinction.

```{r boxplot2-2, fig.height=4, echo=F}
do.call(grid.arrange, args=c(boxp2[boxp.idx2], list(ncol=2)))
```

They got flatter.

To take a look at it from a different angle, let's create density plots. Vertical lines show the mean of each distribution.

```{r densityplot2}
showDensplt <- function(df, x, cls='quality.f2', lower=0, upper=.99) {
  # Compute mean of distribution per level of cls
  mu <- summarise_(group_by_(ww, cls), paste0('mean(', x, ')'))
  colnames(mu)[2] <- x
  
  ggplot(df, aes_string(x=x, fill=cls)) + 
    geom_density(alpha=0.3) +
    geom_vline(aes_string(xintercept=x, color=cls), data=mu, size=1) +
    xlim(quantile(df[, x], prob=lower), 
         quantile(df[, x], prob=upper)) +
    theme(legend.position="none")
}
dens <- lapply(INDEPENDENT, FUN=function(col) showDensplt(ww, col))
do.call(grid.arrange, args=c(dens, list(ncol=3)))
```

The trends and flatness observed in boxplots look like as follows by density plots.

```{r densityplot2-1, fig.height=4, echo=F}
do.call(grid.arrange, args=c(dens[boxp.idx1], list(ncol=2)))
```

The hierarchical distinction observed in the previous boxplots are shown as a wider range of vertical lines.

```{r densityplot2-2, fig.height=4, echo=F}
do.call(grid.arrange, args=c(dens[boxp.idx2], list(ncol=2)))
```

The distribution and mean are very similar among quality levels.

Since it's still hard to find any significance other than `alcohol` and `density`, let's try simpler quality classifications for extreme qualities.
```{r excellent}
# [Quality conversion 2]
# Create factor variables for extreme qualities.
#   excellent: 9,8 -> True
#   inferior:  3,4 -> True
ww$excellent <- ifelse(ww$quality == 8 | ww$quality == 9, T, F)
ww$inferior <- ifelse(ww$quality == 3 | ww$quality == 4, T, F)
summary(ww[, c('excellent', 'inferior')])
```

So can we see any significant independent variable in extreme cases? First take a look at excellet wines.

```{r boxplot3}
boxp3 <- lapply(INDEPENDENT, 
                FUN=function(var) showBoxplt(ww, y=var, x='excellent'))
do.call(grid.arrange, args=c(boxp3, list(ncol=3)))
```

While the gap in `alcohol` and `density` got more significant, not so much difference can be observed in the others. Maybe `pH` of `excellent=TRUE` is slightly bigger than `excellent=FALSE`.

```{r boxplot3-1, fig.height=4, echo=F}
do.call(grid.arrange, args=c(boxp3[boxp.idx1], list(ncol=2)))
```
```{r boxplot3-2, fig.height=4, echo=F}
do.call(grid.arrange, args=c(boxp3[which(INDEPENDENT == 'pH')], 
                             list(ncol=2)))
```

How about inferior wines?

```{r boxplot4}
boxp4 <- lapply(INDEPENDENT, 
                FUN=function(var) showBoxplt(ww, y=var, x='inferior'))
do.call(grid.arrange, args=c(boxp4, list(ncol=3)))
```

What is interesting is that `alcohol` does not have significant difference when it is observed from an inferior viewpoint. Also the difference in `free.sulfur.dioxide` seems to be significant.

```{r boxplot4-1, fig.height=4, echo=F}
boxp.idx3 <- which(INDEPENDENT == 'alcohol' | 
                     INDEPENDENT == 'free.sulfur.dioxide')
do.call(grid.arrange, args=c(boxp4[rev(boxp.idx3)], list(ncol=2)))
```

Those observations imply that:

1. Excellent wines tend to have high alcohol volume, but high alcohol volume does not promise to be excellent.
2. Infeior wines tend to have lower free.sulfur.dioxide, but low free.sulfur.dioxide does not promise to be inferior.


## 4.3. Multivariate Analysis

So now let's turn into higher dimensional space. The idea here is that one variable is visualized by color so that other two variables can be mapped on 2D space. We might be able to find certain rules displayed by color on 2D space.

First, let's try a higher dimensional version of scatter plot. Since the strongest factor to determine `quality` we found so far is `alcohol`, I'd like to map `alcohol` and `quality` on 2D axis and one independent variable into color. Each independent variable used for color is scaled in the following color scheme in its own data range after removing top 1 % as outliers.

- Low = Blue
- Mid = Violet
- High = Red

```{r scatterplot1}
showScatpltColored <- function(df, color, x='alcohol', y='quality',
                          lower=0.00, upper=0.99) {
  ggplot(df, aes_string(x=x, y=y, color=color)) +
    geom_point(size=1, alpha=0.5, position='jitter') + 
    xlim(quantile(df[, x], prob=lower), 
         quantile(df[, x], prob=upper)) +
    scale_colour_gradientn(colours=c("blue", "violet", "red")) +
    ggtitle(color) +
    xlab(NULL) +
    ylab(NULL) +
    theme(legend.position="none")
}
scat_wc <- lapply(INDEPENDENT, FUN=function(var) showScatpltColored(ww, var))
do.call(grid.arrange, args=c(scat_wc, list(ncol=3)))
```

Since `alcohol` is mapped on x-axis and `quality` is mapped on y-axis, horizontal color distinction is related to `alcohol` and vertical color distinction is related to `quality`. So what we want to find is vertical distinction in a sense. Unfortunately, we cannot see vertical color distinction in any plot, although horizontal color distinction is observed in `alcohol` (which is obvious) and `density` (which is also expected because of the correlation with alcohol).

Next, let's convert `quality` into colors. Meaning, arbitrary two independent variables are mapped on 2D axis. Since I don't know which combination might lead a better insight, I'm going to check all the combinations. What we want to find here is any chunck of color on 2D. 

```{r scatterplot2}
showScatpltColored2 <- function(df, x, y, color='quality.f2',
                           lower=0.00, upper=0.99){
  ggplot(df, aes_string(x=x, y=y, color=color)) +
    geom_point(alpha=0.3, position='jitter') + 
    xlim(quantile(df[, x], prob=lower), 
         quantile(df[, x], prob=upper)) +
    ylim(quantile(df[, y], prob=lower), 
         quantile(df[, y], prob=upper))
}
scat_wc2 <- lapply(combn(INDEPENDENT, 2, simplify=F), 
                   FUN=function(var) showScatpltColored2(ww, var[1], var[2]))
# Just check the number of plots
length(scat_wc2)
```

After checking 55 scatter plots that are all the combinations of independent variables (11 choose 2), I noticed that an interesting pattern is observed in a scatter plot by `density` and `residual.sugar`. Let's zoom up the plot.

```{r scatterplot3, fig.height=4}
# Colored by simplified quality level
scat.qf2 <- ggplot(ww, aes(x=density, y=residual.sugar, color=quality.f2)) +
  geom_point(position='jitter') + 
  xlim(min(ww$density), quantile(ww$density, prob=0.99)) +
  ylim(min(ww$residual.sugar), quantile(ww$residual.sugar, prob=0.99))
# Colored by original quality level
scat.qf <- ggplot(ww, aes(x=density, y=residual.sugar, color=quality.f)) +
  geom_point() + 
  scale_color_brewer() +
  xlim(min(ww$density), quantile(ww$density, prob=0.99)) +
  ylim(min(ww$residual.sugar), quantile(ww$residual.sugar, prob=0.99))

do.call(grid.arrange, args=c(list(scat.qf2, scat.qf), list(ncol=2)))
```

The left one uses the simplified quality level and the right one uses the original quality level. In both plots, holding `density`, higher `residual.sugar` seem to have better `quality`. Maybe the distinction is not so deterministic, but it's recognizable in a sense. It seems to be hard to express the effect by a simple linear coeffecient due to the wide spread and overlap of data points.



# 5. Modeling

Based on the exploratory analysis in the previous section, there does not seem to be any simple linear relationship between quality and physicochemical properties. If this observation is correct, linear regression model would not perform so well in terms of quality prediction by physicochemical properties. So let's start there to confirm it.

Before start I'd like to define all formulas used in this section and an utility function to summarize model performance.

```{r formula}
# Original numeric variable prediction
fml1 <- as.formula(paste("quality", "~", 
                         paste(INDEPENDENT, collapse=' + ')))
# As a classification prediction
fml2 <- as.formula(paste("quality.f", "~", 
                         paste(INDEPENDENT, collapse=' + ')))
# As a simpler classification ("bad", "normal", "good") prediction
fml3 <- as.formula(paste("quality.f2", "~", 
                         paste(INDEPENDENT, collapse=' + ')))
# Extreme case detection for excellent ones
fml.e <- as.formula(paste("excellent", "~", 
                          paste(INDEPENDENT, collapse=' + ')))
# Extreme case detection for inferior ones
fml.i <- as.formula(paste("inferior", "~", 
                          paste(INDEPENDENT, collapse=' + ')))
```
```{r describePerformance}
describePerformance <- function(org, pred) {
  cat("[Contingency Table]\n")
  print(table(org, pred))
  cat("\n")
  
  cat("[Contingency Table by Proportion]\n")
  print(round(prop.table(table(org, pred), 1), 3))
  cat("\n")
  
  cat("[Overall Accuracy]\n")
  cat(sum(org == pred) / length(org) * 100, '%')
  cat("\n\n")
  
  cat("[Cohen's kappa]\n")
  kp <- cohen.kappa(cbind(org, pred))
  cat("Unweighted:", kp$kappa, '\n')
  cat("  Weighted:", kp$weighted.kappa)
}
```


## 5.1. Linear Regression

Since this is an extension of exploratory analysis, all data is used for modeling (no validation and test set) and meticulous performance tuning is not performed. For linear regression, all physicochemical properties are used as independent variables first and simply drop one by one whose p-value is not significant (> 0.05).

```{r lm}
m1 <- lm(fml1, ww)
m2 <- update(m1, ~ . - citric.acid)
m3 <- update(m2, ~ . - chlorides)
m4 <- update(m3, ~ . - total.sulfur.dioxide)
mtable(m1, m2, m3, m4)
```

As expected, it's not performing so well. If you look at R-squared in the final model, it's just 0.282 which means that only 28 % of variance of `quality` is explained by those independent variables.

```{r lm evaluated}
cat("RMSE:", sqrt(mean((ww$quality - m4$fitted.values)^2)))
describePerformance(ww$quality, round(m4$fitted.values, 0))
```

The root mean square error (RMSE) is 0.75, meaning the deviation from the true quality value is 0.75 as the average. This does not feel so bad though, the model seems to be struggling to predict deviant cases from the mean quality. Since number of the deviant cases is small, this inability is not reflected fairly to the RMSE. The more sensitive measurement to the inbalance is Cohen's kappa and it shows low accuracy level.

This implies that there is not any simple linear relationship between quality and physicochemical properties so that if a linear regression model is applied to the data, the prediction is dragged to the center value which is the majority and it's hard to predict quality as it goes far from the mean.


## 5.2. Support Vector Machine

So let's try some non-linear models. How does Support Vector Machine perform?


```{r svm1}
# Original numeric variable prediction
svm1 <- svm(fml1, ww)
cat("RMSE:", sqrt(mean((ww$quality - svm1$fitted)^2)))
describePerformance(ww$quality, round(svm1$fitted, 0))
```

Although it's still not performing well for deviant cases, the performance is better than linear regression model. Especially there is a big improvement in Cohen's kappa.

```{r svm2}
# As a classification prediction
svm2 <- svm(fml2, ww)
describePerformance(ww$quality.f, svm2$fitted)
```

When it's converted to a classification prediction, the performance got worse. Maybe it's because rounding off in regression model is working in a good way.

```{r svm3}
# As a simpler classification ("bad", "normal", "good") prediction
svm3 <- svm(fml3, ww)
describePerformance(ww$quality.f2, svm3$fitted)
```

The simpler classification gives the model a slight improvement. But not so much.

```{r svm4}
# Extreme case detection for excellent ones
ww$excellent <- as.factor(ww$excellent)
svm4 <- svm(fml.e, ww)
describePerformance(ww$excellent, svm4$fitted)
```
```{r svm5}
# Extreme case detection for inferior ones
ww$inferior <- as.factor(ww$inferior)
svm5 <- svm(fml.i, ww)
describePerformance(ww$inferior, svm5$fitted)
```

It seems to be very difficult for SVM to predict extreme cases. This implies that there is not any significant distinction for extreme qualities even for the boundry scheme of SVM in the high dimensional space.


## 5.3. Random Forest

Lastly let's see how Random Forest works.

```{r rf1}
# Original numeric variable prediction
set.seed(20160211)
rf1 <- randomForest(fml1, ww)
cat("RMSE:", sqrt(mean((ww$quality - rf1$predicted)^2)))
describePerformance(ww$quality, round(rf1$predicted, 0))
```

Even for Random Forest it is still hard to predict deviant cases. Meanwhile it shows a better performance than SVM overall because the prediction accuracy for center values (5, 6, 7) got better.

```{r rf2}
# As a classification prediction
rf2 <- randomForest(fml2, ww)
describePerformance(ww$quality.f, rf2$predicted)
```

When it's converted to a classification prediction, it outperforms the regression version. Here improvements in the prediction accuracy for quality level 4 and 8 are observed. It feels that the tree-based rule logic and randomness of the model enables to predict some deviant cases although it still cannot output quality level 3 and 9.

```{r rf3}
# As a simpler classification ("bad", "normal", "good") prediction
rf3 <- randomForest(fml3, ww)
describePerformance(ww$quality.f2, rf3$predicted)
```

This is the best performance so far. Simplification of quality level worked well for Random Forest.

```{r rf4}
# Extreme case detection for excellent ones
rf4 <- randomForest(fml.e, ww)
describePerformance(ww$excellent, rf4$predicted)
```

While SVM could not detect any excellent ones, Random Forest could detect about 42 % of them.

```{r rf5}
# Extreme case detection for inferior ones
rf5 <- randomForest(fml.i, ww)
describePerformance(ww$inferior, rf5$predicted)
```

This is slightly better than SVM though, detection rate of inferior ones is about 22 %.



# 6. Final Plots and Summary
As a summary, I'm going to revisit findings with plots.

## 6.1. Alcohol
```{r final1}
ggplot(ww, aes(x=quality.f2, y=alcohol, fill=quality.f2)) + 
  geom_boxplot() + 
  stat_summary(fun.data=mean_cl_normal, geom="errorbar", 
               color='red', size=1) +
  ylim(quantile(ww$alcohol, prob=0.00), 
       quantile(ww$alcohol, prob=0.99)) +
  ggtitle("Range of Alcohol per Quality Level") + 
  xlab("Quality") +
  ylab("Alcohol (%)") +
  theme(title=element_text(size=18, face="bold"),
        axis.title=element_text(size=14), 
        axis.text=element_text(size=12),
        legend.position="none")
```

The 95 % confidence intervals are added by red lines that have relatively narrow range because of large numbers of data points. It shows that mean of each group are significantly different at that confidence level. This can imply that higher alcohol volume produce better quality wine although the causality cannot be clear only from this data analysis. (It is unclear whether high alcohol volume directly contributes to better quality.)

## 6.2. Density
```{r final2}
# Compute mean of distribution per level of cls
mu <- summarise(group_by(ww, quality.f2), mean(density))
colnames(mu)[2] <- "density"

ggplot(ww, aes(x=density, fill=quality.f2)) + 
  geom_density(alpha=0.3) +
  geom_vline(aes(xintercept=density, color=quality.f2), 
             data=mu, size=1, show.legend=F) +
  xlim(quantile(ww$density, prob=0.00), 
       quantile(ww$density, prob=0.99)) +
  guides(fill=guide_legend(title="Quality", reverse=T)) +
  ggtitle("Distribution of Density per Quality Level") + 
  xlab("Density (g/ml)") +
  ylab("") +
  theme(title=element_text(size=18, face="bold"),
      axis.title=element_text(size=14), 
      axis.text=element_text(size=12),
      legend.text=element_text(size=12))
```

This is a density plot of `density`. As `quality` goes up, the center of distribution of `density` gets smaller. Since `density` has a high correlation coeffecient with `alcohol`, this is something expected. The lower `density` possibly produce better quality wine although the causality is not deterministic here as well. (This might be just due to the negative correlation (`r round(cor(ww$alcohol, ww$density), 3)`) between `alcohol` and `density`.)


## 6.3. Residual Sugar with Density
```{r final3}
ggplot(ww, aes(x=density, y=residual.sugar, color=quality.f2)) +
  geom_point(position='jitter') + 
  geom_smooth(aes(color=quality.f2), method=lm) +
  xlim(min(ww$density), 
       quantile(ww$density, prob=0.99)) +
  ylim(min(ww$residual.sugar), 
       quantile(ww$residual.sugar, prob=0.99)) +
  guides(color=guide_legend(title="Quality", reverse=T)) +
  ggtitle("Quality on Density and Residual Suagr Dimension") + 
  xlab("Density (g/mL)") +
  ylab("Residual Sugar (g/L)") +
  theme(title=element_text(size=18, face="bold"),
      axis.title=element_text(size=14), 
      axis.text=element_text(size=12),
      legend.text=element_text(size=12))
```

A regression line is added to each quality group with the shadow of confidence interval. When `density` is low, the distinction among the quality levels is clear, but as `density` increases the margin gets narrower and at the high end they cross. This means that the distinction in the 2D space is only applicable where `density` is low to middle.

Through exploratory data analysis, I found that `alcohol`, `density` and `residual.sugar` are possible factors which affect quality level in an interpretable manner. Our sense of taste feels quite complex and the result of modeling implies that accurate prediction of wine quality requires more complexity which is not interpretable through visualization in a few dimensional space.


# 7. Reflection
When I started out this project, I thought that it would not be so difficult to find hidden relationships between quality and physicochemical properties because the data set has only eleven independent variables and all of them are clean. But I soon noticed I was wrong after performing some visualizations through arbitrary variable choices. What I came up with ater the struggle was a brute force method which plots all the combinations of variables. This way I could check if there is any interesting pattern in different type of 2D spaces effeciently. By systemizing recursive plotting through concise function calls, I also could minimize the amount of code. The plot in the section 6.3 is one of them that I could find by the brute force method and would have been difficult to be found otherwise.

I came up with new quality level representations to make more sense for visualization. Those are transformation of the dependent variable and it worked. But I could not utilize transformation of independent variables and feature engineering. Based on the observation in the modeling section, there is a tendency that more complex non-linear models perform better. But number of models I applied in this project is limitted and model tuning was not performed. So those things related to modeling are left for the future project.



# 8. Reference
- R Markdown Reference Guide - http://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf
- Knitr with R Markdown, knitr in a knutshell - http://kbroman.org/knitr_knutshell/pages/Rmarkdown.html
- Chunk options and package options, knitr - http://yihui.name/knitr/options/
- Set theme elements, ggplot2 2.0.0 - http://docs.ggplot2.org/current/theme.html


