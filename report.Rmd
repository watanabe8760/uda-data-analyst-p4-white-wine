---
title: "Quality and Physicochemical Properties of White Wine"
author: "Teru Watanabe"
date: "Thursday, February 11, 2016"
output: html_document
---

### Table of Contents
1. Abstract
2. Data
3. Library
4. Exploratory Analysis  
    4.1. Univariate Analysis  
    4.2. Bivariate Analysis  
    4.3. Multivariate Analysis  
5. Modeling  
    5.1. Linear Regression  
    5.2. Support Vector Machine  
    5.3. Random Forest  
6. Final Plots and Summary  
    6.1. Alcohol  
    6.2. Density  
    6.3. Residual Sugar with Density  
7. Reflection
8. Reference


# 1. Abstract
The purpose of this project is to determine what physicochemical properties affect white wine quality through exploratory data analysis. 



# 2. Data
The Wine Quality dataset is downloaded from the following website.

[Website] http://www3.dsi.uminho.pt/pcortez/wine/  
[Data Link] http://www3.dsi.uminho.pt/pcortez/wine/winequality.zip  
[Citation] P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.

The zip file contains two csv files, one for red wines (winequality-red.csv) and one for white wines (winequality-white.csv). In this project only the white wine data is used.

The white wine csv file is loaded into R as follows. 
```{r}
# Load csv file
ww <- read.table("./input/winequality-white.csv", header=T, sep=";")
str(ww)
```
The first eleven variables are physicochemical properties acquired from a computerized system iLab and the last variable `quality` is the score between 0 (very bad) and 10 (excellent) based on sensory data. Each sample was evaluated by a minimum of three sensory assessors (using blind tastes) and the final sensory score was given by the median of these evaluations.


For a sake of convenience of analysis, original variable names and independent variable names (physicochemical properties) are defined.
```{r}
# Define original variable names
ORIGINAL <- colnames(ww)
# Define independent variable names
INDEPENDENT <- colnames(ww)[1:11]
```

To enable to look at the quality as a classification, a factor variable which is converted from the numerical one is added.
```{r}
ww$quality.f <- as.factor(ww$quality)
```



# 3. Library
The following libraries are used in this project.
```{r, message=F, warning=F}
# For visualization
library(ggplot2)
library(gridExtra)
library(corrplot)
# For modeling
library(psych)
library(memisc)
library(nnet)
library(rpart)
library(e1071)
library(randomForest)
```



# 4. Exploratory Analysis
## 4.1. Univariate Analysis
```{r}
summary(ww)
```

All variables except `residual.sugar` have very similar median and mean. This makes us imagine symetrical distributions. Also, by looking at 1st quarter and 3rd quarter, it feels that they are relatively close to the center values. So how do they look like?

```{r, out.width='\\maxwidth'}
dens <- lapply(ORIGINAL, FUN=function(var) {
  ggplot(ww, aes_string(x=var)) + 
    geom_density(fill='gray')
  })
do.call(grid.arrange, args=c(dens, list(ncol=3)))
```

As expected, most of them, except `residual.sugar` and `alcohol`, have symetrical distribution althoguh some have outliers in positive side. The dependent variiable `quality` is a little bit exceptional since it's a discrete number variable.

To have a better sense about the distributions, let's standardize and see.
```{r}
ww_scaled <- data.frame(scale(ww[, ORIGINAL]))
summary(ww_scaled)
```

Since the 1st and 3rd quarter of standard normal distribution is `r round(qnorm(0.25), 3)` and `r round(qnorm(0.75), 3)` respectively, some of them look really close to it. So let's check the normality by Q-Q plot.

```{r, out.width='\\maxwidth'}
par(mfrow=c(4,3), mar=c(2,2,2,2))
dummy <- lapply(ORIGINAL, FUN=function(var) {
  qqnorm(ww_scaled[, var], main=var)
  qqline(ww_scaled[, var])
})
```

Frankly speaking, all of them follow normal distribution in the duration between -2 and 2 standard deviations. (Again, `quality` is exceptional since it's a discrete number distribution.) But as it gets close to the end of positive, outliers are observed. Only `alcohol` has narrower distribution in the positive side as compared to normal distribution. On the other hand, if you look at negative sides, most of them has narrower distiribution than normal distribution. Those observations imply that distributions of independent variables are close to normal distribution at the center, but outliers in positive sides add a taste of right skewness.


## 4.2. Bivariate Analysis
To take an overview of relationships between two variables, let's start with correlation matrix.
```{r}
corrplot.mixed(cor(ww[, ORIGINAL]))
```

In terms of relationships between `quality` and independent variables, `alcohol` has the strongest correlation 0.44 with `quality`. The second is `density` -0.31 though, it seems natural since the correlation between `density` and `alcohol` is -0.78. (The `density` does not seem to provide any new information about quality since it's already explained by alcohol.)

In terms of relationships between independent variables, some strong correlations are observed.

* 0.84 `residual.sugar` - `density`
* 0.62 `free.sulfur.dioxide` - `total.sulfur.dioxide`
* 0.53 `total.sulfur.dioxide` - `density`

To see relationships between `quality` and independent variables closely, let's create boxplots which desceribe each independent variable per quality level. Here I remove outliers in the positive side to visualize the mojority clearer.

```{r, warning=F, out.width='\\maxwidth'}
showBoxplt <- function(df, y, x='quality.f', lower=0, upper=.99) {
  ggplot(df, aes_string(x=x, y=y, fill=x)) + 
    geom_boxplot() + 
    ylim(quantile(df[, y], prob=lower), 
         quantile(df[, y], prob=upper)) +
    theme(legend.position="none")
}
boxp <- lapply(INDEPENDENT, FUN=function(var) showBoxplt(ww, var))
do.call(grid.arrange, args=c(boxp, list(ncol=3)))
```

By looking at the center lines and position of boxes, we can feel the correlation in `alcohol` and `density` which we found in the correlation matrix. As `quality` goes up, `alcohol` also goes up and `density` goes down. Meanwhile the others look almost flat.

Since it's hard to find significant factor at this point, let's create a new variable which has fewer quality levels and see if it can provide new insights.
```{r}
# [Quality conversion 1]
# Create a simpler classification of quality.
#   3, 4, 5 -> bad
#      6    -> normal
#   7, 8, 9 -> good
ww$quality.f2 <- ifelse(ww$quality == 3 | 
                          ww$quality == 4 | 
                          ww$quality == 5, 
                        "bad", 
                        ifelse(ww$quality == 6, 
                               "normal",
                               "good"))
ww$quality.f2 <- factor(ww$quality.f2, levels=c("bad", "normal", "good"))
summary(ww[, c('quality.f', 'quality.f2')])
```

So now how do boxplots look like?

```{r, warning=F, out.width='\\maxwidth'}
boxp <- lapply(INDEPENDENT, 
               FUN=function(var) showBoxplt(ww, var, 'quality.f2'))
do.call(grid.arrange, args=c(boxp, list(ncol=3)))
```

While significant differences among quality levels in `density` and `alcohol` got clearer, the others got flatter. This feels like that the other physicochemical properties do not really matter for quality.

Just to make sure, let's look at the new quality variable through density plot.
```{r, warning=F, out.width='\\maxwidth'}
showDensplt <- function(df, x, cls='quality.f2', lower=0, upper=.99) {
  ggplot(df, aes_string(x=x, fill=cls)) + 
    geom_density(alpha=0.3) +
    xlim(quantile(df[, x], prob=lower), 
         quantile(df[, x], prob=upper))
}
dens <- lapply(INDEPENDENT, FUN=function(col) showDensplt(ww, col))
do.call(grid.arrange, args=c(dens, list(ncol=3)))
```

One thing I did not notice when I saw the boxplots above is that `chlorides` has the similar distribution to `density`. It was difficult to notice because of the skewness.

Since it's still hard to find any significance other than `alcohol` and `density`, let's try simpler quality classifications for extreme qualities.
```{r}
# [Quality conversion 2]
# Create factor variables for extreme qualities.
#   excellent: 9,8 -> True
#   inferior:  3,4 -> True
ww$excellent <- ifelse(ww$quality == 8 | ww$quality == 9, T, F)
ww$inferior <- ifelse(ww$quality == 3 | ww$quality == 4, T, F)
summary(ww[, c('excellent', 'inferior')])
```

So can we see any significant independent variable in extreme cases?

```{r, warning=F, out.width='\\maxwidth'}
boxp <- lapply(INDEPENDENT, 
               FUN=function(var) showBoxplt(ww, y=var, x='excellent'))
do.call(grid.arrange, args=c(boxp, list(ncol=3)))
```

While the gap in `alcohol` and `density` got more significant, not so much difference can be observed in the others. Maybe pH of `excellent=TRUE` is slightly bigger than `excellent=FALSE`.

```{r, warning=F, out.width='\\maxwidth'}
boxp <- lapply(INDEPENDENT, 
               FUN=function(var) showBoxplt(ww, y=var, x='inferior'))
do.call(grid.arrange, args=c(boxp, list(ncol=3)))
```

What is interesting is that `alcohol` does not have significant difference when it is observed from an inferior viewpoint. Also the difference in `free.sulfur.dioxide` seems to be significant.

Those observations imply that:  
1. Excellent wines tend to have high alcohol volume, but high alcohol volume does not promise to be excellent.
2. Infeior wines tend to have lower free.sulfur.dioxide, but low free.sulfur.dioxide does not promise to be inferior.


## 4.3. Multivariate Analysis

So now let's turn into higher dimentional space. The idea here is that one variable is visualized by color so that other two variables can be mapped on 2D space. We might be able to find certain rules displayed by color on 2D space.

First, let's try a higher dimentional version of scatter plot. Since the strongest factor to determine `quality` we found so far is `alcohol`, I'd like to map `alcohol` and `quality` on 2D axis and one independent variable into color. Each independent variable used for color is scaled in the following color scheme in its own data range after removing top 1 % as outliers.

- Low = Blue
- Mid = Violet
- High = Red

```{r, warning=F, out.width='\\maxwidth'}
showScatpltColored <- function(df, color, x='alcohol', y='quality',
                          lower=0.00, upper=0.99) {
  ggplot(df, aes_string(x=x, y=y, color=color)) +
    geom_point(size=1, alpha=0.5, position='jitter') + 
    xlim(quantile(df[, x], prob=lower), 
         quantile(df[, x], prob=upper)) +
    scale_colour_gradientn(colours=c("blue", "violet", "red")) +
    ggtitle(color) +
    xlab(NULL) +
    ylab(NULL) +
    theme(legend.position="none")
}
scat_wc <- lapply(INDEPENDENT, FUN=function(var) showScatpltColored(ww, var))
do.call(grid.arrange, args=c(scat_wc, list(ncol=3)))
```

Since `alcohol` is mapped on x-axis and `quality` is mapped on y-axis, horizontal color distinction is related to `alcohol` and vertical color distinction is related to `quality`. So what we want to find is vertical distinction in a sense. Unfortunately, we cannot see vertical color distinction in any plot, although horizontal color distinction is observed in `alcohol` (which is obvious) and `density` (which is also expected because of the correlation with alcohol).

Next, let's convert `quality` into colors. Meaning, arbitrary two independent variables are mapped on 2D axis. Since I don't know which combination might lead a better insight, I'm going to check all the combinations. What we want to find here is any chunck of color on 2D. Note, I use the simpler quality classification ('bad', 'normal', 'good') to make it easy.

```{r, out.width='\\maxwidth'}
showScatpltColored2 <- function(df, x, y, color='quality.f2',
                           lower=0.00, upper=0.99){
  ggplot(df, aes_string(x=x, y=y, color=color)) +
    geom_point(alpha=0.3, position='jitter') + 
    xlim(quantile(df[, x], prob=lower), 
         quantile(df[, x], prob=upper)) +
    ylim(quantile(df[, y], prob=lower), 
         quantile(df[, y], prob=upper))
}
scat_wc2 <- lapply(combn(INDEPENDENT, 2, simplify=F), 
                   FUN=function(var) showScatpltColored2(ww, var[1], var[2]))
length(scat_wc2)
```

Since it produces 55 scatter plots, I'm not showing them. After checking those scatter plots, I noticed that an interesting rule is observed in a scatter plot by `density` and `residual.sugar`. Let's zoom up the plot.

```{r, warning=F}
ggplot(ww, aes(x=density, y=residual.sugar, color=quality.f2)) +
  geom_point(position='jitter') + 
  xlim(min(ww$density), quantile(ww$density, prob=0.99)) +
  ylim(min(ww$residual.sugar), quantile(ww$residual.sugar, prob=0.99))
```

Holding `density`, higher `residual.sugar` seem to have better `quality`. Maybe the distinction is not so deterministic, but it's recognizable in a sense.

How does it look like if the original quality level is applied to the color scheme? 

```{r, warning=F}
ggplot(ww, aes(x=density, y=residual.sugar, color=quality)) +
  geom_point() + 
  xlim(min(ww$density), quantile(ww$density, prob=0.99)) +
  ylim(min(ww$residual.sugar), quantile(ww$residual.sugar, prob=0.99)) +
  scale_color_gradientn(colors=rainbow(7))
```

The rule found in the simplified quality level feels held, but the dominance of middle levels makes it unclear. So let's separate the scatter plot by quality level.

```{r, warning=F}
scat_per_qual <- lapply(9:3, FUN=function(n) {
  ggplot(subset(ww, quality == n), 
         aes(x=density, y=residual.sugar)) +
    geom_point(color=rainbow(7)[n-2]) + 
    ggtitle(paste("Quality", n)) + 
    xlim(min(ww$density), quantile(ww$density, prob=0.99)) +
    ylim(min(ww$residual.sugar), quantile(ww$residual.sugar, prob=0.99))
})
do.call(grid.arrange, args=c(scat_per_qual, list(ncol=3)))
```

The impression has slightly changed. Although the layer created by quality 8 and 4 gives us the same impression, the spread of middle levels (7, 6, 5) and sparsity of quality 3 make us think that this finding might be just a chance due to the lack of data for extreme cases.

Even if the finding is true, it seems to be hard to express the effect by a simple linear coeffecient due to the wide spread of data point in one level.



# 5. Modeling

Based on the exploratory analysis in the previous section, there does not seem to be any simple linear relationship between quality and physicochemical properties. If this observation is correct, linear regression model would not perform so well in terms of quality prediction by physicochemical properties. So let's start there to confirm it.

Before start I'd like to define all formulas used in this section and an utility function to summarize model performance.

```{r}
# Original numeric variable prediction
fml1 <- as.formula(paste("quality", "~", 
                         paste(INDEPENDENT, collapse=' + ')))
# As a classification prediction
fml2 <- as.formula(paste("quality.f", "~", 
                         paste(INDEPENDENT, collapse=' + ')))
# As a simpler classification ("bad", "normal", "good") prediction
fml3 <- as.formula(paste("quality.f2", "~", 
                         paste(INDEPENDENT, collapse=' + ')))
# Extreme case detection for excellent ones
fml.e <- as.formula(paste("excellent", "~", 
                          paste(INDEPENDENT, collapse=' + ')))
# Extreme case detection for inferior ones
fml.i <- as.formula(paste("inferior", "~", 
                          paste(INDEPENDENT, collapse=' + ')))
```
```{r}
describePerformance <- function(org, pred) {
  cat("[Contingency Table]\n")
  print(table(org, pred))
  cat("\n")
  
  cat("[Contingency Table by Proportion]\n")
  print(round(prop.table(table(org, pred), 1), 3))
  cat("\n")
  
  cat("[Overall Accuracy]\n")
  cat(sum(org == pred) / length(org) * 100, '%')
  cat("\n\n")
  
  cat("[Cohen's kappa]\n")
  kp <- cohen.kappa(cbind(org, pred))
  cat("Unweighted:", kp$kappa, '\n')
  cat("  Weighted:", kp$weighted.kappa)
}
```


## 5.1. Linear Regression

Since this is an extension of exploratory analysis, all data is used for modeling (no validation and test set) and meticulous performance tuning is not performed. For linear regression, all physicochemical properties are used as independent variables first and simply drop one by one whose p-value is not significant (> 0.05).

```{r}
m1 <- lm(fml1, ww)
m2 <- update(m1, ~ . - citric.acid)
m3 <- update(m2, ~ . - chlorides)
m4 <- update(m3, ~ . - total.sulfur.dioxide)
mtable(m1, m2, m3, m4)
```

As expected, it's not performing so well. If you look at R-squared in the final model, it's just 0.282 which means that only 28 % of variance of `quality` is explained by those independent variables.

```{r}
cat("RMSE:", sqrt(mean((ww$quality - m4$fitted.values)^2)))
describePerformance(ww$quality, round(m4$fitted.values, 0))
```

The root mean square error (RMSE) is 0.75, meaning the deviation from the true quality value is 0.75 as the average. This does not feel so bad though, the model seems to be struggling to predict deviant cases from the mean quality. Since number of the deviant cases is small, this inability is not reflected fairly to the RMSE. The more sensitive measurement to the inbalance is Cohen's kappa and it shows low accuracy level.

This implies that there is not any simple linear relationship between quality and physicochemical properties so that if a linear regression model is applied to the data, the prediction is dragged to the center value which is the majority and it's hard to predict quality as it goes far from the mean.


## 5.2. Support Vector Machine

So let's try some non-linear models. How does Support Vector Machine perform?


```{r}
# Original numeric variable prediction
svm1 <- svm(fml1, ww)
cat("RMSE:", sqrt(mean((ww$quality - svm1$fitted)^2)))
describePerformance(ww$quality, round(svm1$fitted, 0))
```

Although it's still not performing well for deviant cases, the performance is better than linear regression model. Especially there is a big improvement in Cohen's kappa.

```{r}
# As a classification prediction
svm2 <- svm(fml2, ww)
describePerformance(ww$quality.f, svm2$fitted)
```

When it's converted to a classification prediction, the performance got worse. Maybe it's because rounding off in regression model is working in a good way.

```{r}
# As a simpler classification ("bad", "normal", "good") prediction
svm3 <- svm(fml3, ww)
describePerformance(ww$quality.f2, svm3$fitted)
```

The simpler classification gives the model a slight improvement. But not so much.

```{r}
# Extreme case detection for excellent ones
ww$excellent <- as.factor(ww$excellent)
svm4 <- svm(fml.e, ww)
describePerformance(ww$excellent, svm4$fitted)
```
```{r}
# Extreme case detection for inferior ones
ww$inferior <- as.factor(ww$inferior)
svm5 <- svm(fml.i, ww)
describePerformance(ww$inferior, svm5$fitted)
```

It seems to be very difficult for SVM to predict extreme cases. This implies that there is not any significant distinction for extreme qualities even for the boundry scheme of SVM in the high dimentional space.


## 5.3. Random Forest

Lastly let's see how Random Forest works.

```{r}
# Original numeric variable prediction
set.seed(20160211)
rf1 <- randomForest(fml1, ww)
cat("RMSE:", sqrt(mean((ww$quality - rf1$predicted)^2)))
describePerformance(ww$quality, round(rf1$predicted, 0))
```

Even for Random Forest it is still hard to predict deviant cases. Meanwhile it shows a better performance than SVM overall because the prediction accuracy for center values (5, 6, 7) got better.

```{r}
# As a classification prediction
rf2 <- randomForest(fml2, ww)
describePerformance(ww$quality.f, rf2$predicted)
```

When it's converted to a classification prediction, it outperforms the regression version. Here improvements in the prediction accuracy for quality level 4 and 8 are observed. It feels that the tree-based rule logic and randomness of the model enables to predict some deviant cases although it still cannot output quality level 3 and 9.

```{r}
# As a simpler classification ("bad", "normal", "good") prediction
rf3 <- randomForest(fml3, ww)
describePerformance(ww$quality.f2, rf3$predicted)
```

This is the best performance so far. Simplification of quality level worked well for Random Forest.

```{r}
# Extreme case detection for excellent ones
rf4 <- randomForest(fml.e, ww)
describePerformance(ww$excellent, rf4$predicted)
```

While SVM could not detect any excellent ones, Random Forest could detect about 42 % of them.

```{r}
# Extreme case detection for inferior ones
rf5 <- randomForest(fml.i, ww)
describePerformance(ww$inferior, rf5$predicted)
```

This is slightly better than SVM though, detection rate of inferior ones is about 22 %.



# 6. Final Plots and Summary
As a summary, I'm going to revisit findings with plots.

## 6.1. Alcohol
```{r, warning=F}
ggplot(ww, aes(x=quality.f2, y=alcohol, fill=quality.f2)) + 
  geom_boxplot() + 
  ylim(quantile(ww$alcohol, prob=0.00), 
       quantile(ww$alcohol, prob=0.99)) +
  xlab("Quality") +
  ylab("Alcohol") +
  theme(legend.position="none")
```

Generally speaking better quality ones have more alcohol volume. However, if the range of whiskers (tails of distribution) is included, the range of alcohol is almost the same for all quality levels. So only `alcohol` cannot be a deterministic factor of quality.

## 6.2. Density
```{r, warning=F}
ggplot(ww, aes(x=density, fill=quality.f2)) + 
  geom_density(alpha=0.3) +
  xlim(quantile(ww$density, prob=0.00), 
       quantile(ww$density, prob=0.99)) +
  xlab("Density") +
  ylab("") +
  guides(fill=guide_legend(title="Quality", reverse=T))
```

This is a density plot of `density`. As `quality` goes up, the center of distribution of `density` gets smaller. Since `density` has a high correlation coeffecient with `alcohol`, this is something expected. A large area of overlapping implies that only `density` cannot be a deterministic factor of quality as well.


## 6.3. Residual Sugar with Density
```{r, warning=F}
ggplot(ww, aes(x=density, y=residual.sugar, color=quality.f2)) +
  geom_point(position='jitter') + 
  xlim(min(ww$density), 
       quantile(ww$density, prob=0.99)) +
  ylim(min(ww$residual.sugar), 
       quantile(ww$residual.sugar, prob=0.99)) +
  xlab("Density") +
  ylab("Residual Sugar") +
  guides(color=guide_legend(title="Quality", reverse=T))
```

Hoding density, better quality ones seem to have higher `residual.sugar`. Again, the overlapping area is large and it's not deterministic though. At the high end of density, blue dots are relatively located lower position so the rule does not seem to be applicable.

Through exploratory data analysis, I found that `alcohol`, `density` and `residual.sugar` are possible factors which affect quality level though, none of them are deterministic. 


# 7. Reflection
When I started out this project, I thought that it would not be so difficult to find hidden relationships between quality and physicochemical properties because the data set has only eleven independent variables and all of them are clean. But I soon noticed I was wrong after performing some visualizations through arbitrary variable choices. What I came up with ater the struggle was a brute force method which plots all the combination or pattern of variables. This way I could check if there is any interesting pattern in different type of 2D spaces effeciently. By systemizing recursive plotting through concise function calls, I also could minimize the amount of code. The plot in the section 6.3 is one of them that I could find by the brute force method and would have been difficult to be found otherwise.

I came up with new quality level representations to make more sense for visualization. Those are transformation of the dependent variable and it worked. But I could not utilize transformation of independent variables and feature engineering. Based on the observation in the modeling section, there is a tendency that more complex non-linear models perform better. But number of models I applied in this project is limitted and model tuning was not performed. So those things related to modeling are left for the future project.



# 8. Reference
- R Markdown Reference Guide - http://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf


